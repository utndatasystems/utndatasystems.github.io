---
layout: post
title: The Advent of Semantic Data Processing Systems
date: 2025-12-16
description: Semantic query processing is emerging as a new layer atop relational engines, elevating LLM-backed semantic operators to first-class SQL primitives for multimodal data. We present SemBench, the first benchmark to rigorously evaluate these systems end-to-end, and outline our roadmap toward Spectra to make semantic operators viable at scale.
related_publications: true
toc:
  sidebar: right

author: "<a href=\"https://andi-zimmerer.com/\">Andi Zimmerer</a>"
---


Today, I have the pleasure (or obligation) of introducing two new things:

 - First, very visibly, the blog you are currently reading. This post is intended to be the first in a series on research topics from the Data Systems Lab at UTN.

 - Second, I would like to introduce the SemBench project {% cite lao2025sembench %}, which we have been working on in collaboration with Cornell, TU Berlin/BIFOLD, the University of Michigan, MIT CSAIL, Vrije Universiteit Amsterdam, and Google.

SemBench fills a gap we have observed in the benchmarking world: it is a benchmark dedicated to multimodal semantic query processing systems.
Although you could view these systems as a mix of traditional database systems and modern multimodal question-answering systems (like some LLMs, including ChatGPT), and there is a plethora of existing benchmarks in each respective domain, none of these benchmarks is suitable for semantic query processing systems.

## What are Semantic Query Processing Systems?

_...and why are they needed?_

Traditional database systems excel at processing (mostly relational) data at scale.
They come with their own query language, usually SQL, that allows users to describe what the result of a query should look like. This comes with advantages and disadvantages:
while the semantics are well defined and allow for a number of optimizations, the set of supported operations is somewhat limited.
Numerical data is supported as a first-class citizen; strings usually also work very well, but face additional challenges (indexes and collations, whoops).
However, when it comes to more complex data types, like images, documents, or even audio files, users quickly hit limitations and are left with the following options:

 1. Take data out of the database, process it externally in application code, and put it back. This loses all kinds of consistency guarantees, and external processing scripts are usually not optimized for efficient processing.
 2. If the database system supports it, users can write a UDF in a different language (e.g., Python) with access to libraries. This works well, but UDFs are notoriously hard for database systems to optimize and are usually treated as black boxes.
 3. Not use a database system.

Options (1) and (3) are undesirable in practice; (2) is workable but suboptimal.

In the past few years, a second player that is very good at simple yet extremely flexible data processing on unstructured data across multiple modalities has emerged: multimodal LLMs.
They excel at helping novice users make sense of unstructured data by merely describing what they want in natural language.
They can interpret images, transcribe audio, summarize documents, answer complex questions over existing data, etc.
Of course, sometimes their results are not accurate, but in many cases, that might be okay.

In fact, these systems are so flexible and relatively fast at processing almost arbitrary data that we -- and many others -- think it would be really tempting if they could just crunch through our huge data collections like we can do with SQL databases.
Today that isn’t viable because of context limits, but per-record calls are feasible.
In fact, it’s just a simple API call that can be wrapped in a UDF if the system supports it. Great!

With this, it is possible to build _semantic operators_:

 - A function `AI_FILTER()` takes a natural-language predicate and a tuple as context, evaluates the predicate with an LLM and extracts a boolean value from its response, allowing users to filter data. Placed on top of a (cross-)join, this even allows users to join records semantically.
 - `AI_SCORE()` rates a tuple based on a natural-language description and emits a numerical score, allowing a database system to sort by that number.
 - `AI_CLASSIFY()` assigns class labels to records based on class descriptions, making it usable in `GROUP BY` clauses (or traditional labeling).
 - `AI_AGG()` takes multiple tuples at once as much as the model’s context window allows and returns an aggregate value, for example a summary across multiple scanned document pages.
 - etc.

These primitives allow for very flexible but well-defined queries with LLM operations on data records in a database, unlocking a vast array of new analytical capabilities in database systems.
While all of these operations could be implemented as UDFs, baking them into a query engine allows for much more advanced optimizations that would otherwise be very hard, or even impossible, to implement.
The idea of semantic query processing engines is to extend SQL to make these functions first-class citizens.

An example that demonstrates the flexible processing capabilities of such systems could be:

```sql
SELECT
  -- Classify papers into different areas
  AI_CLASSIFY('Assign the paper to one of the following areas', {
    'efficient-filters': 'Papers discussing how semantic filters can be implemented more efficiently',
    'efficient-joins': 'Papers discussing how semantic joins can be implemented more efficiently',
    'other': 'Everything else',
  }) as area,
  -- Summarize state of the art
  AI_AGG('Summarize the research field based on the papers: {p.file}') as summary
FROM LIST_FILES('./arxiv_downloads/*.pdf') as p
WHERE true
  -- Filter papers based on research field
  AND AI_FILTER('Is the paper {p.file} about semantic database engines?')
GROUP BY area
-- Order summaries by how advanced the field appears
ORDER BY AI_SCORE('How advanced is the research field based on the summary: {summary}');
```

Surprisingly, despite semantic query processing still being a young topic, there are already a number of academic research systems—and even industry systems—that support semantic query processing.
Among research systems, we have LOTUS ([GitHub](https://github.com/lotus-data/lotus), [paper](https://arxiv.org/abs/2407.11418)), Palimpzest ([GitHub](https://github.com/mitdbg/palimpzest), [paper](https://arxiv.org/abs/2505.14661)), ThalamusDB ([GitHub](https://github.com/itrummer/thalamusdb), [paper](https://par.nsf.gov/servlets/purl/10577787)), FlockMTL ([GitHub](https://github.com/dais-polymtl/flock), [paper](https://dl.acm.org/doi/10.14778/3750601.3750685)), and CAESURA ([paper](https://arxiv.org/pdf/2308.03424)).
Additionally, major industry players like Snowflake ([paper](https://arxiv.org/pdf/2511.07663), [blog post](https://www.snowflake.com/en/engineering-blog/cortex-aisql-query-optimization/)) and BigQuery ([blog post](https://cloud.google.com/blog/products/data-analytics/sql-reimagined-for-the-ai-era-with-bigquery-ai-functions)) have already integrated semantic functions in their offerings.


The idea of extending SQL with semantic operators is, in fact, not new.
More than a decade ago, a research area on "crowd-sourced" database engines was established.
Back then, human workers on crowd-working platforms were hired to answer simple record-labeling or comparison tasks during query execution.
The latency was huge, costs were extraordinary, and human errors had to be mitigated.
Research back then focused on human error and making certain operators more efficiently executable; but things like semantic embeddings and other metadata that captures the relationship between items were not exploited back then.
Nowadays, we can replace humans with LLMs for these simple tasks, making them faster and cheaper -- however, still prohibitively expensive and slow for large datasets as we observed with SemBench.


## Why SemBench?

To evaluate semantic query engines, we need benchmarks that test both  _accuracy loss due to optimizations_ and _end-to-end execution efficiency_ in terms of cost and latency.

Traditional database benchmarks like the TPC family or JOB are great because they focus on large-scale relational data processing, but they lack any semantic operations.
Extending these benchmarks with semantic operators is also not feasible because the underlying datasets (a) usually lack unstructured data types, including textual descriptions or images, and (b) provide no ground truth for these semantic queries.

Benchmarks in the ML area, on the other hand, usually focus on assessing the correctness of the answer to a single question: classifying data points, identifying objects in an image, multimodal question answering, etc.
Even though these questions are usually repeated thousands of times and averages are reported, there is no focus on techniques that efficiently answer these questions over a dataset as a whole.

Neither of the two sides is really suitable to benchmark and compare the semantic query processing systems that have been developed so far, so the idea of implementing a new benchmark specifically for these systems came up at a [Dagstuhl Seminar in 2025](https://www.dagstuhl.de/seminars/seminar-calendar/seminar-details/25182). This is how SemBench {% cite lao2025sembench %} was born.


### Dataset and Workloads of SemBench
SemBench consists of five use cases, each with 10-14 queries, resulting in a total of 55 queries comprising 78 semantic operators.
Each use case comes with its own dataset and query patterns with a verifiable ground truth.

Use cases cover a range of different modalities, including tabular data, textual descriptions, images, and audio.
A breakdown of which use case contains which modalities can be found in the following table (taken from the paper):

-----

| Scenario   |Table | Text | Image | Audio |
|------------|:----:|:----:|:-----:|:-----:|
| Movie      | ✓ | ✓ | – | – |
| Wildlife   | ✓ | – | ✓ | ✓ |
| E-Comm     | ✓ | ✓ | ✓ | – |
| MMQA       | ✓ | ✓ | ✓ | – |
| Medical    | ✓ | ✓ | ✓ | ✓ |

-----


We also tried to cover as many different specialized semantic operators as possible under the constraint of a verifiable ground truth.
Naturally, there is a strong focus on `AI_FILTER` and also `AI_JOIN`.
A breakdown of the number of different semantic operators per use case can be found in the following table (also taken from the paper):

-----

| Scenario   | #Queries | `AI_FILTER` | `AI_JOIN` | `AI_MAP` | `AI_SCORE` | `AI_CLASSIFY` |
|------------|---:|------------:|----------:|---------:|-----------:|--------------:|
| Movie      | 10 | 4  | 3  | – | 2 | 1 |
| Wildlife   | 10 | 17 | –  | – | – | – |
| E-Comm     | 14 | 12 | 9  | 3 | 1 | 2 |
| MMQA       | 11 | 5  | 3  | 4 | – | – |
| Medical    | 10 | 12 | –  | – | – | 1 |
| **Total**  | **55** | **49** | **15** | **7** | **3** | **4** |

-----

We implemented SemBench scenarios for LOTUS, Palimpzest, ThalamusDB, and BigQuery; but we encourage other vendors to contribute their results.
To compare results and identify strengths and weaknesses of each system, we set up a website with a leaderboard that we plan to update for new submissions: <http://sembench.org/>.
Note that all of the systems are currently under active development and the reported metrics are merely a snapshot in time.
We hope to see better numbers soon!

The reference implementations together with the benchmarking queries and dataset references can be found in the SemBench repository on GitHub: <https://github.com/SemBench/SemBench>.



### Ground Truth vs. "Gold Standard Ground Truth"

During the development of the benchmark, we faced the question of what to focus the ground truth on in order to compute query accuracy.
In general, there are two common approaches:

 - In the machine learning domain, ground truth is usually "absolute" ground truth -- results are compared against labels that have been determined by humans. The advantage is that model performance can be evaluated in absolute terms for specific tasks.
 - Another option is to compare results to a "gold standard" model: the best available model used as a baseline for these operators. This yields relative accuracy, measuring whether optimizations stay close to baseline quality while reducing cost (e.g., ~99% relative accuracy). Its key advantage is broad applicability, since it requires no human-labeled ground truth and works for any dataset or query.

Both options are viable, but the latter has the disadvantage of being a moving target:
Improvements in ML models could render earlier relative accuracy scores invalid.
Therefore, we opted for absolute ground truth for SemBench.



## Research Roadmap

With our collaboration on SemBench, we decided to dive into semantic query processing and started building our own system under the name "[Spectra]({% link _projects/spectra.md %})".

Our system aims to solve a major limitation we observed during SemBench: semantic operators are, as of now, hardly usable on large-scale datasets.
While they are a valuable addition to SQL, their applications are somewhat limited due to relatively high costs and slow processing times.
We want to take on that challenge and make semantic operators feasible for large-scale datasets.
Stay tuned!
