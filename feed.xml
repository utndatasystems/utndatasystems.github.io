<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://utndatasystems.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://utndatasystems.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-25T13:34:34+00:00</updated><id>https://utndatasystems.github.io/feed.xml</id><title type="html">UTN Data Systems</title><subtitle></subtitle><entry><title type="html">Benchmarking Semantic Query Processing Systems</title><link href="https://utndatasystems.github.io/blog/2026/sembench/" rel="alternate" type="text/html" title="Benchmarking Semantic Query Processing Systems"/><published>2026-02-16T00:00:00+00:00</published><updated>2026-02-16T00:00:00+00:00</updated><id>https://utndatasystems.github.io/blog/2026/sembench</id><content type="html" xml:base="https://utndatasystems.github.io/blog/2026/sembench/"><![CDATA[<p>Semantic database systems bring probabilistic, LLM-based operators into traditionally deterministic relational engines for query processing. This integration introduces a fundamental tradeoff between result quality and execution cost. Yet, existing benchmarks do not evaluate this tradeoff at the system level.</p> <p>In this post, we present <a href="http://sembench.org/">SemBench</a>, a benchmark designed to rigorously assess semantic query processing systems end-to-end.</p> <p>SemBench fills a gap we have observed in the benchmarking world: it is a benchmark specifically dedicated to multimodal semantic query processing systems. These systems integrate modern multimodal question-answering systems into traditional database engines for query processing. Although there are numerous existing benchmarks in each respective domain, we found none of them to be suitable for these new kinds of systems, motivating the development of SemBench.</p> <p>The SemBench project is in collaboration with researchers and practitioners from Cornell University, UTN, BIFOLD &amp; TU Berlin, University of Michigan, MIT CSAIL, Vrije Universiteit Amsterdam, and Google: <a href="https://scholar.google.de/citations?user=PccHINcAAAAJ">Jiale Lao</a>, <a href="https://scholar.google.de/citations?user=jBVA5UsAAAAJ">Andi Zimmerer</a>, <a href="https://scholar.google.de/citations?user=JnbLQ6wAAAAJ">Olga Ovcharenko</a>, <a href="https://scholar.google.de/citations?user=3ToRqNMAAAAJ">Tianji Cong</a>, <a href="https://scholar.google.de/citations?user=HYrDojkAAAAJ">Matthew Russo</a>, <a href="https://scholar.google.de/citations?user=zYBXv3sAAAAJ">Gerardo Vitagliano</a>, <a href="https://scholar.google.de/citations?user=JuZrOtoAAAAJ">Michael Cochez</a>, <a href="https://scholar.google.de/citations?user=JzoifEMAAAAJ">Fatma Özcan</a>, <a href="https://www.linkedin.com/in/guptagautam/">Gautam Gupta</a>, <a href="https://www.linkedin.com/in/thibaudhottelier/">Thibaud Hottelier</a>, <a href="https://scholar.google.de/citations?user=SKVnHakAAAAJ">H. V. Jagadish</a>, <a href="https://www.linkedin.com/in/kris-kissel-b69b15b0/">Kris Kissel</a>, <a href="https://scholar.google.de/citations?user=zCpQUukAAAAJ">Sebastian Schelter</a>, <a href="https://scholar.google.de/citations?user=y3IdRusAAAAJ">Andreas Kipf</a>, and <a href="https://scholar.google.de/citations?user=FJ7im7AAAAAJ">Immanuel Trummer</a>.</p> <h2 id="what-are-semantic-query-processing-systems">What are Semantic Query Processing Systems?</h2> <p><em>…and why are they needed?</em></p> <p>Semantic query processing systems bring the magically flexible data processing capabilities of multimodal LLMs (e.g., GPT-4o) into semantically well-defined traditional database systems by adding new so-called <em>semantic operators</em>.</p> <h3 id="current-state">Current State</h3> <p>Traditional database systems excel at processing data at scale. They rely on their own declarative query language, typically SQL, which has both benefits and limitations: While the semantics are well-defined and allow for a number of impactful optimizations, the set of supported operations is limited by the available functionality of SQL. Numerical data is supported as a first-class citizen; strings usually also work very well. However, when it comes to more complex data types, like images, documents, or even audio files, traditional database systems typically treat them as opaque BLOBs and users are left with the following options:</p> <ol> <li>Take data out of the database, process it externally in application code, and put it back. This loses consistency guarantees, and external processing scripts are usually not optimized for efficient data handling.</li> <li>If supported by the database system, users can write User-Defined Functions (UDFs) in a different language (e.g., Python) with access to libraries. This works well, but UDFs are notoriously hard for database systems to optimize and are usually treated as black boxes.</li> <li>Avoid database system altogether.</li> </ol> <p>Options (1) and (3) are undesirable in practice; (2) is workable but suboptimal.</p> <h3 id="extending-sql-with-llms">Extending SQL with LLMs</h3> <p>Modern multimodal LLMs are particularly well suited for processing such unstructured BLOB data types. In addition, they enable non-technical users to interact with and analyze this data by expressing their intent directly in natural language without the need of advanced programming skills. Examples include <em>“Summarize the content of this audio-interview”</em> or <em>“Is there an elephant in the picture?”</em>.</p> <p>Because LLMs can flexibly interpret unstructured inputs with reasonable latency, it is tempting to integrate them into workflows that process large data collections, much like relational databases do for structured data. However, throwing a large table at an LLM is not directly possible due to table sizes typically exceeding the LLM’s context limits; but record-level processing is possible, albeit slow and expensive. In fact, record-level processing is as simple as an API call plus some additional post-processing inside a UDF.</p> <p>This simple pattern (LLM call plus response parsing) can be formalized into reusable query primitives <a href="https://arxiv.org/pdf/2407.11418">(Patel et al., 2024)</a>, including:</p> <ul> <li>A function <code class="language-plaintext highlighter-rouge">AI_FILTER()</code> takes a natural-language predicate and a tuple as context, evaluates the predicate with an LLM and extracts a boolean value from its response, allowing users to filter data. Placed on top of a (cross-)join, this even allows users to join records semantically.</li> <li><code class="language-plaintext highlighter-rouge">AI_SCORE()</code> rates a tuple based on a natural-language description and emits a numerical score, allowing a database system to sort by that number.</li> <li><code class="language-plaintext highlighter-rouge">AI_CLASSIFY()</code> assigns class labels to records based on class descriptions, making it usable in <code class="language-plaintext highlighter-rouge">GROUP BY</code> clauses (or traditional labeling).</li> <li><code class="language-plaintext highlighter-rouge">AI_AGG()</code> takes as many tuples as the model’s context window allows and returns an aggregate value, for example a summary across multiple scanned document pages.</li> </ul> <p>These primitives allow for flexible but well-defined queries with LLM operations on rows in a database, unlocking a vast array of new analytical capabilities in database systems. All of these operators are, by nature, fuzzy and might produce different results depending on the LLM being used.</p> <p>While all of these operations <em>could</em> be implemented as UDFs, a deeper integration into a query engine allows for much more advanced optimizations, like model cascades <a href="https://arxiv.org/pdf/2405.14696">(Liu et al., 2024)</a>, exploitation of item-similarity <a href="https://www.eecs.umich.edu/courses/cse584/static_files/papers/Semantic_operators.pdf">(Patel et al., 2025)</a>, prompt fusion, prompt compression <a href="https://arxiv.org/pdf/2405.14696">(Liu et al., 2024)</a>, and predicate pull-up. The idea of semantic query processing engines is to realize this deep integration, making semantic functions first-class citizens in database systems. Similarly, SemBench is designed to expose the impact of these optimizations.</p> <p>The following example query demonstrates the flexibility of semantic query processing systems, making use of <code class="language-plaintext highlighter-rouge">AI_FILTER</code>, <code class="language-plaintext highlighter-rouge">AI_CLASSIFY</code>, <code class="language-plaintext highlighter-rouge">AI_SCORE</code>, and <code class="language-plaintext highlighter-rouge">AI_AGG</code>:</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="k">SELECT</span>
  <span class="c1">-- Classify papers into different areas</span>
  <span class="n">AI_CLASSIFY</span><span class="p">(</span><span class="s1">'Assign the paper {p.file} to one of the following areas'</span><span class="p">,</span> <span class="p">{</span>
    <span class="s1">'efficient-filters'</span><span class="p">:</span> <span class="s1">'Papers discussing how semantic filters can be implemented more efficiently'</span><span class="p">,</span>
    <span class="s1">'efficient-joins'</span><span class="p">:</span> <span class="s1">'Papers discussing how semantic joins can be implemented more efficiently'</span><span class="p">,</span>
    <span class="s1">'other'</span><span class="p">:</span> <span class="s1">'Everything else'</span><span class="p">,</span>
  <span class="p">})</span> <span class="k">as</span> <span class="n">area</span><span class="p">,</span>
  <span class="c1">-- Summarize state of the art</span>
  <span class="n">AI_AGG</span><span class="p">(</span><span class="s1">'Summarize the research field based on the papers: {p.file}'</span><span class="p">)</span> <span class="k">as</span> <span class="n">summary</span>
<span class="k">FROM</span> <span class="n">LIST_FILES</span><span class="p">(</span><span class="s1">'./arxiv_downloads/*.pdf'</span><span class="p">)</span> <span class="k">as</span> <span class="n">p</span>
<span class="k">WHERE</span> <span class="k">true</span>
  <span class="c1">-- Filter papers based on research field</span>
  <span class="k">AND</span> <span class="n">AI_FILTER</span><span class="p">(</span><span class="s1">'Is the paper {p.file} about semantic database engines?'</span><span class="p">)</span>
<span class="k">GROUP</span> <span class="k">BY</span> <span class="n">area</span>
<span class="c1">-- Order summaries by how advanced the field appears</span>
<span class="k">ORDER</span> <span class="k">BY</span> <span class="n">AI_SCORE</span><span class="p">(</span><span class="s1">'How advanced is the research field based on the summary: {summary}'</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div> <h3 id="existing-implementations">Existing Implementations</h3> <p>Surprisingly, despite semantic query processing still being a young topic, there are already a large number of academic research systems—and even industry systems—that support semantic query processing. Among research systems, there is LOTUS (<a href="https://github.com/lotus-data/lotus">GitHub</a>, <a href="https://arxiv.org/abs/2407.11418">paper</a>), Palimpzest (<a href="https://github.com/mitdbg/palimpzest">GitHub</a>, <a href="https://arxiv.org/abs/2505.14661">paper</a>), ThalamusDB (<a href="https://github.com/itrummer/thalamusdb">GitHub</a>, <a href="https://par.nsf.gov/servlets/purl/10577787">paper</a>), FlockMTL (<a href="https://github.com/dais-polymtl/flock">GitHub</a>, <a href="https://dl.acm.org/doi/10.14778/3750601.3750685">paper</a>), CAESURA (<a href="https://github.com/DataManagementLab/caesura">GitHub</a>, <a href="https://arxiv.org/pdf/2308.03424">paper</a>), and BlendSQL (<a href="https://github.com/parkervg/blendsql">GitHub</a>, <a href="https://aclanthology.org/2024.findings-acl.25.pdf">paper</a>). Additionally, major industry players like Snowflake (<a href="https://arxiv.org/pdf/2511.07663">paper</a>, <a href="https://www.snowflake.com/en/engineering-blog/cortex-aisql-query-optimization/">blog post</a>), BigQuery (<a href="https://cloud.google.com/blog/products/data-analytics/sql-reimagined-for-the-ai-era-with-bigquery-ai-functions">blog post</a>), and Databricks (<a href="https://docs.databricks.com/aws/en/large-language-models/ai-functions">documentation</a>) have already integrated semantic functions in their offerings.</p> <h2 id="why-sembench">Why SemBench?</h2> <p>To evaluate semantic query engines, we need benchmarks that test both <em>accuracy loss due to optimizations</em> and <em>end-to-end execution efficiency</em> in terms of cost and latency.</p> <p>A benchmark should help answer the following questions:</p> <ol> <li>Can semantic operators be optimized without significant accuracy degradation?</li> <li>How does cost scale with dataset size?</li> <li>How stable are results across models?</li> <li>What is the impact of specific optimizations?</li> </ol> <p>Traditional database benchmarks like the <a href="https://www.tpc.org/tpch/">TPC family</a> or JOB <a href="https://15721.courses.cs.cmu.edu/spring2020/papers/22-costmodels/p204-leis.pdf">(Leis et al., 2015)</a> focus on large-scale relational data processing, but lack any semantic operations. Extending these benchmarks with semantic operators is also not feasible because the underlying datasets (a) usually lack unstructured data types, including textual descriptions or images, and (b) provide no ground truth to verify the results of semantic queries.</p> <p>Benchmarks in the ML area, on the other hand, usually focus on assessing the correctness of the answer to a single question: classifying data points, identifying objects in an image, multimodal question answering, etc. Even though these questions are usually repeated thousands of times and averages are reported, there is no focus on techniques that efficiently answer these questions over a large dataset as a whole.</p> <p>Neither of the two benchmarking worlds is really suitable to assess semantic query processing systems, so the idea of implementing a new benchmark specifically for these systems came up at a <a href="https://www.dagstuhl.de/seminars/seminar-calendar/seminar-details/25182">Dagstuhl Seminar in 2025</a>. This is how SemBench <a class="citation" href="#lao2025sembench">(Lao et al., 2025)</a> was born.</p> <h3 id="dataset-and-workloads-of-sembench">Dataset and Workloads of SemBench</h3> <p>SemBench consists of five use cases, each with 10-14 queries, resulting in a total of 55 queries comprising 78 semantic operators. Each use case comes with its own dataset and query patterns with a verifiable ground truth.</p> <p>Use cases cover a range of different modalities, including tabular data, textual descriptions, images, and audio. A breakdown of which use case contains which modalities can be found in the following table (taken from the paper):</p> <hr/> <table> <thead> <tr> <th>Scenario</th> <th style="text-align: center">Table</th> <th style="text-align: center">Text</th> <th style="text-align: center">Image</th> <th style="text-align: center">Audio</th> </tr> </thead> <tbody> <tr> <td>Movie</td> <td style="text-align: center">✓</td> <td style="text-align: center">✓</td> <td style="text-align: center">–</td> <td style="text-align: center">–</td> </tr> <tr> <td>Wildlife</td> <td style="text-align: center">✓</td> <td style="text-align: center">–</td> <td style="text-align: center">✓</td> <td style="text-align: center">✓</td> </tr> <tr> <td>E-Comm</td> <td style="text-align: center">✓</td> <td style="text-align: center">✓</td> <td style="text-align: center">✓</td> <td style="text-align: center">–</td> </tr> <tr> <td>MMQA</td> <td style="text-align: center">✓</td> <td style="text-align: center">✓</td> <td style="text-align: center">✓</td> <td style="text-align: center">–</td> </tr> <tr> <td>Cars</td> <td style="text-align: center">✓</td> <td style="text-align: center">✓</td> <td style="text-align: center">✓</td> <td style="text-align: center">✓</td> </tr> </tbody> </table> <hr/> <p>We also tried to cover as many different specialized semantic operators as possible under the constraint of a verifiable ground truth. Naturally, there is a strong focus on <code class="language-plaintext highlighter-rouge">AI_FILTER</code> and also <code class="language-plaintext highlighter-rouge">AI_JOIN</code>. A breakdown of the number of different semantic operators per use case can be found in the following table:</p> <hr/> <table> <thead> <tr> <th>Scenario</th> <th style="text-align: right">#Queries</th> <th style="text-align: right"><code class="language-plaintext highlighter-rouge">AI_FILTER</code></th> <th style="text-align: right"><code class="language-plaintext highlighter-rouge">AI_JOIN</code></th> <th style="text-align: right"><code class="language-plaintext highlighter-rouge">AI_MAP</code></th> <th style="text-align: right"><code class="language-plaintext highlighter-rouge">AI_SCORE</code></th> <th style="text-align: right"><code class="language-plaintext highlighter-rouge">AI_CLASSIFY</code></th> </tr> </thead> <tbody> <tr> <td>Movie</td> <td style="text-align: right">10</td> <td style="text-align: right">4</td> <td style="text-align: right">3</td> <td style="text-align: right">–</td> <td style="text-align: right">2</td> <td style="text-align: right">1</td> </tr> <tr> <td>Wildlife</td> <td style="text-align: right">10</td> <td style="text-align: right">17</td> <td style="text-align: right">–</td> <td style="text-align: right">–</td> <td style="text-align: right">–</td> <td style="text-align: right">–</td> </tr> <tr> <td>E-Comm</td> <td style="text-align: right">14</td> <td style="text-align: right">12</td> <td style="text-align: right">9</td> <td style="text-align: right">3</td> <td style="text-align: right">1</td> <td style="text-align: right">2</td> </tr> <tr> <td>MMQA</td> <td style="text-align: right">11</td> <td style="text-align: right">5</td> <td style="text-align: right">3</td> <td style="text-align: right">4</td> <td style="text-align: right">–</td> <td style="text-align: right">–</td> </tr> <tr> <td>Cars</td> <td style="text-align: right">10</td> <td style="text-align: right">12</td> <td style="text-align: right">–</td> <td style="text-align: right">–</td> <td style="text-align: right">–</td> <td style="text-align: right">1</td> </tr> <tr> <td><strong>Total</strong></td> <td style="text-align: right"><strong>55</strong></td> <td style="text-align: right"><strong>49</strong></td> <td style="text-align: right"><strong>15</strong></td> <td style="text-align: right"><strong>7</strong></td> <td style="text-align: right"><strong>3</strong></td> <td style="text-align: right"><strong>4</strong></td> </tr> </tbody> </table> <hr/> <p>We implemented SemBench scenarios for LOTUS, Palimpzest, ThalamusDB, and BigQuery; but we encourage other vendors to contribute their results. To compare results and identify strengths and weaknesses of each system, we set up a leaderboard that we plan to update as new submissions arrive: <a href="http://sembench.org/">http://sembench.org/</a>. Note that all of the systems are currently under active development and the reported metrics are merely a snapshot in time.</p> <p>The reference implementations together with the benchmarking queries and dataset references can be found in the SemBench repository on GitHub: <a href="https://github.com/SemBench/SemBench">https://github.com/SemBench/SemBench</a>.</p> <h3 id="ground-truth-vs-gold-standard-ground-truth">Ground Truth vs. “Gold Standard Ground Truth”</h3> <p>During the development of the benchmark, we faced the question of what to base the ground truth on in order to compute query accuracy. In general, there are two common approaches:</p> <ul> <li>In the machine learning domain, ground truth is usually “absolute” ground truth: results are compared against labels that have been determined by humans. The advantage is that model performance can be evaluated in absolute terms for specific tasks.</li> <li>Another option is to compare results to a “gold standard” model: the best available model used as a baseline for these operators. This yields relative accuracy, measuring whether optimizations stay close to baseline quality while reducing cost (e.g., ~99% relative accuracy). Its key advantage is broad applicability, since it requires no human-labeled ground truth and works for any dataset or query.</li> </ul> <p>Both options are viable, but the latter has the disadvantage of being a moving target: improvements in ML models could render earlier relative accuracy scores invalid. Therefore, we opted for absolute ground truth in SemBench.</p> <h3 id="call-for-contributions">Call for Contributions</h3> <p>With this post, we not only introduce SemBench but also invite contributions to the <a href="http://sembench.org/">online leaderboard</a>.</p> <p>The landscape of semantic query processing systems is evolving rapidly: new systems continue to emerge, and existing ones are improved at a fast pace. To ensure that SemBench remains representative and up to date, we encourage system developers to implement the benchmark for their own systems and submit their results.</p> <p>We are currently formalizing a structured contribution process. In the meantime, researchers and practitioners interested in contributing results are welcome to contact us directly.</p> <h2 id="utns-research-roadmap">UTN’s Research Roadmap</h2> <p>During the work on SemBench, we observed that semantic operators are not yet practical on large-scale datasets due to their extraordinarily high monetary costs and slow processing times. We decided to tackle this limitation, making it a core research focus of our own semantic query processing engine <a href="/projects/spectra/">Spectra</a>.</p>]]></content><author><name>&lt;a href=&quot;https://andi-zimmerer.com/&quot;&gt;Andi Zimmerer&lt;/a&gt;</name></author><summary type="html"><![CDATA[Semantic query processing is emerging as a new layer atop relational engines, elevating LLM-backed semantic operators to first-class SQL primitives for multimodal data. We present SemBench, the first benchmark to rigorously evaluate these systems end-to-end, and outline our roadmap towards our own system, Spectra, to make semantic operators affordable at scale.]]></summary></entry><entry><title type="html">Democratizing Data Science</title><link href="https://utndatasystems.github.io/blog/2026/dataloom/" rel="alternate" type="text/html" title="Democratizing Data Science"/><published>2026-01-16T00:00:00+00:00</published><updated>2026-01-16T00:00:00+00:00</updated><id>https://utndatasystems.github.io/blog/2026/dataloom</id><content type="html" xml:base="https://utndatasystems.github.io/blog/2026/dataloom/"><![CDATA[<p>The database community often focuses on improving the performance of database systems, which is (and always will be) of critical importance. This continues to drive innovation by unlocking ever larger datasets, challenging workloads, and near-instantaneous results. Yet, people outside the database community struggle to harness the treasure trove of modern database systems: It takes hours and expert knowledge to set up, load, and run workloads; thus wasting time and delaying insights. Therefore, the database community has started widening its focus, as highlighted in a recent industry perspective:</p> <blockquote> <p><em>“The important feature of a database is how quickly you can go from idea to answer, not query to result.”</em> – January, 2024, MotherDuck, <strong>Jordan Tigani</strong></p> </blockquote> <p>and at last year’s SIGMOD panel discussion:</p> <blockquote> <p><em>“We are asking the world to bring their data into our format. We need to work on the entire data science pipeline!”</em> – June, 2025, SIGMOD Berlin, <strong>Sihem Amer-Yahia</strong></p> </blockquote> <p>In this article, we explore why this new frontier matters and present Dataloom, our research prototype designed to help open the black box of data systems to a broader audience.</p> <h2 id="new-metrics-time-to-insight-and-accessibility">New Metrics: Time-To-Insight and Accessibility</h2> <p>The task of a data scientist is to answer questions based on data. Given a dataset, this involves many steps before an answer can be formulated: understanding the data, loading it, cleaning it, writing queries, visualizing results, and usually many iterations in each step. Sharply put, a database that processes terabytes in milliseconds is useless to a user who cannot figure out how to load their data into it. In more technical terms, we can consider <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s law</a>: If it takes an hour to understand, load, and clean data, the benefit of speeding up the runtime of a 1-second query by 10x is rather limited. In these interactive scenarios, <strong>time-to-insight</strong> (i.e., finding an answer to a particular question) is critical. Improving this process enables users to answer questions while they are still relevant and allows shifting employee cycles from tedious tasks to the interesting and fun parts of data science.</p> <p>At the same time, working with data is becoming omnipresent in professional as well as personal life. Sticking with well-known metaphors: Data shares not only the value of oil but also its fluid nature, seeping into all domains and across skill levels. Nowadays, most aspects of an organization (controlling, reporting, monitoring, and research) heavily rely on data. Even individually, people track finances, health, fitness, and more. In both settings, an increasing number of people, especially non-experts, want to work with data. Improving the ease-of-use of well-studied, schema-rich, and scalable data processing systems (i.e., database systems) can thus benefit a broad spectrum of people (even veteran data scientists know how much time is spent wrangling with formats or missing values).</p> <p>TL;DR: time-to-insight is an important metric for more and more people.</p> <h2 id="case-study-data-engineering-class-of-2023">Case Study: Data Engineering Class of 2023</h2> <p>Our Master’s students at <a href="https://www.utn.de/en/">UTN</a> have a data engineering class where they learn to build scalable data pipelines. As part of their final project, we ran TPC-H in <a href="https://aws.amazon.com/redshift/">Amazon Redshift</a> and gave them a dump of all log files in CSV format. Their task was to figure out which workload we ran and prepare a dashboard with their analysis.</p> <p>The first major hurdle for the students was loading the dataset into an analytical database. They had to figure out which CSV files belonged to which table, infer the schema and column names, create the SQL <code class="language-plaintext highlighter-rouge">create table</code> statements, and then load the data into the system. The result: Lots of frustration, copious amounts of “uninspired” boilerplate code for loading, and delayed results.</p> <h2 id="enter-dataloom">Enter: Dataloom</h2> <p>On the very same day, we hacked together the first version of Dataloom: A simple prototype that—in its earliest form—combined LLMs and classical algorithms (e.g., CSV parsing, type inference, etc.) to make the loading process seamless and fast. <em>How did it do this?</em> Given a chaotic set of files, we use an LLM to figure out which files belong to which table, how to name columns, and create a natural language description of the table—all tasks that used to require human intervention, as traditional algorithms are very bad at this. We then ran some classical schema detection algorithms and loaded everything into <a href="https://duckdb.org/">DuckDB</a>. Thus we transformed an hour-long process that stood between students and their weekend into a quick and easy-to-use tool … that worked 80% of the time ;).</p> <p>Building Dataloom, we quickly realized that LLMs can be unpredictable at times and don’t always churn out the correct results in the intended way. However, we were not daunted but instead inspired to make Dataloom into a system that allows users easy access to data analytics. Initially, we focused on the schema mapping and refinement issue, which we presented in a <a href="https://www.vldb.org/pvldb/vol17/p4449-renen.pdf">demo paper at VLDB 2024</a>, which sparked much interest in the attendees. Over the previous year, Dataloom has expanded in its scope from just loading data to cleaning, visualizing, and reporting results—thus encapsulating more and more data science tasks.</p> <p>Our vision is to build an end-to-end agentic data platform, enabling domain experts to acquire, clean, analyze, and visualize data in a principled manner by combining the benefits of LLMs with decades of database research.</p> <p>Stay tuned for more posts on our <a href="https://utndatasystems.github.io/data-loom/">Dataloom website</a>: “Future (agentic) data is not doomed, it will be loomed.”</p>]]></content><author><name>&lt;a href=&quot;https://dblp.org/pid/219/9679.html&quot;&gt;Alexander van Renen&lt;/a&gt; and &lt;a href=&quot;https://scholar.google.de/citations?user=y3IdRusAAAAJ&amp;hl=en&quot;&gt;Andreas Kipf&lt;/a&gt;</name></author><summary type="html"><![CDATA[Our vision is to build an end-to-end agentic data platform, enabling domain experts to acquire, clean, analyze, and visualize data in a principled manner by combining the benefits of LLMs with decades of database research.]]></summary></entry><entry><title type="html">Launching Our Blog And Wrapping Up 2025</title><link href="https://utndatasystems.github.io/blog/2025/recap/" rel="alternate" type="text/html" title="Launching Our Blog And Wrapping Up 2025"/><published>2025-12-31T00:00:00+00:00</published><updated>2025-12-31T00:00:00+00:00</updated><id>https://utndatasystems.github.io/blog/2025/recap</id><content type="html" xml:base="https://utndatasystems.github.io/blog/2025/recap/"><![CDATA[<p>I’m super excited to launch our blog! We’ll use this space to share what’s happening in our lab, from research papers and systems to the day-to-day life of our team. To kick things off, let’s look back at 2025.</p> <h2 id="award-winning-research">Award-Winning Research</h2> <p>We were super happy to see our work recognized by the community this year!</p> <ul> <li><strong>SIGMOD 2025 Honorable Mention</strong>: Our paper <a href="https://arxiv.org/pdf/2409.08013">“DPconv: Super-Polynomially Faster Join Ordering”</a> by Mihail Stoian and Andreas Kipf received an Honorable Mention at SIGMOD 2025 in Berlin.</li> <li><strong>EDBT 2025 Best Demo Award</strong>: We were also thrilled to pick up a <strong>Best Demo Award</strong> at EDBT 2025 in Barcelona for <a href="https://utndatasystems.github.io/virtual/">“Virtual: Compressing Data Lake Files”</a>.</li> </ul> <h2 id="collaborations">Collaborations</h2> <p>Research in data systems doesn’t happen in an ivory tower. We love working closely with industry leaders and top academic labs to solve real-world problems.</p> <ul> <li><strong>SIGMOD 2025 Industrial</strong>: We presented <a href="https://snowflakepruning.github.io/">“Pruning in Snowflake: Working Smarter, Not Harder”</a> in collaboration with Snowflake, where we explored some cool new pruning techniques.</li> <li><strong>Google BigQuery &amp; SemBench</strong>: Our work on <a href="https://sembench.ngrok.io/"><strong>SemBench</strong></a>, a benchmark for semantic query processing engines, is a huge team effort with Google BigQuery, Cornell, TU Berlin, University of Michigan, MIT CSAIL, and Vrije Universiteit Amsterdam.</li> </ul> <h2 id="a-benchmark-heavy-year">A Benchmark-Heavy Year</h2> <p>Benchmarks are the foundation of systems research. While we’ve been involved in benchmarking for a while (you might know our past work on <a href="https://arxiv.org/pdf/1809.00677"><strong>JOB-light</strong></a>, <a href="https://github.com/learnedsystems/SOSD"><strong>SOSD</strong></a>, and <a href="https://github.com/amazon-science/redset"><strong>Redset</strong></a>), we’ve ramped up our efforts in 2025:</p> <ul> <li><a href="https://arxiv.org/pdf/2506.12488"><strong>Redbench</strong></a>: We presented it at aiDM 2025, and we’re continuing to develop this workload synthesis work in collaboration with <strong>TU Darmstadt</strong>.</li> <li><a href="http://sembench.org/"><strong>SemBench</strong></a>: Our new benchmark, designed specifically for evaluating semantic SQL operators.</li> </ul> <h2 id="community-building-and-interdisciplinary-research">Community Building and Interdisciplinary Research</h2> <p>Beyond the core research, we’ve been connecting with the broader community:</p> <ul> <li><strong>Bavarian Database Day</strong>: We’re proud to have organized the very first <a href="https://databaseday.de/"><strong>Bavarian Database Day</strong></a>, which brought together researchers and practitioners from Bavaria and beyond.</li> <li><strong>Podcast Feature</strong>: Mihail Stoian hopped on the <strong>Disseminate podcast</strong> to talk about our work on robust query execution in the <a href="https://www.youtube.com/watch?v=HfZuVtY5lZg"><strong>episode on Parachute</strong></a>.</li> <li><strong>UTN Internal Milestone</strong>: We even published a first <a href="https://arxiv.org/abs/2507.10391"><strong>cross-department workshop paper</strong></a>, which was a big step for interdisciplinary research here at UTN.</li> </ul> <h2 id="everything-else">Everything Else!</h2> <p>It wouldn’t be a proper recap without mentioning all the other cool stuff we did:</p> <ul> <li><strong>BTW 2025</strong>: I was co-organizing the <a href="https://luthramanisha.github.io/ML4Sys-and-Sys4ML/"><strong>ML4Sys and Sys4ML workshop</strong></a> at BTW 2025, and I also gave a talk about <a href="https://itu-dasyalab.github.io/btw2025workshop/">“Workload-Driven Indexing in the Cloud”</a>.</li> <li><strong>Dagstuhl Seminar</strong>: I attended a very cool Dagstuhl seminar on <a href="https://www.dagstuhl.de/seminars/seminar-calendar/seminar-details/25182"><strong>Table Representation Learning</strong></a>, which is actually where we first kicked off our work on SemBench.</li> <li><strong>Lab Offsite</strong>: We had a nice summer offsite south of Nuremberg. Between working on <strong>ML-based data compression</strong>, we managed to squeeze in some fun swimming and wakeboarding at the nearby lake.</li> <li><strong>VLDB Presentations</strong>: We were also busy in London at VLDB. We presented <a href="https://arxiv.org/abs/2507.10391">“Instance-Optimized String Fingerprints”</a> at AIDB and <a href="https://www.arxiv.org/pdf/2506.13670"><strong>Parachute</strong></a> at the main Research Track.</li> <li><strong>Long Night of Sciences</strong>: We presented our agentic data analytics platform <a href="https://utndatasystems.github.io/data-loom/"><strong>DataLoom</strong></a> and our data lake file format <a href="https://utndatasystems.github.io/virtual/"><strong>Virtual</strong></a> at the <a href="https://nacht-der-wissenschaften.de/"><strong>Long Night of Sciences</strong></a>.</li> </ul> <p>All in all, it’s been a crazy year, and I couldn’t be more proud of what we’ve achieved. I’m super excited for what 2026 has in store. Stay tuned!</p>]]></content><author><name>&lt;a href=&quot;https://scholar.google.de/citations?user=y3IdRusAAAAJ&amp;hl=en&quot;&gt;Andreas Kipf&lt;/a&gt;</name></author><summary type="html"><![CDATA[I'm super excited to launch our blog! We'll use this space to share what's happening in our lab, from research papers and systems to the day-to-day life of our team. To kick things off, let's look back at 2025.]]></summary></entry></feed>